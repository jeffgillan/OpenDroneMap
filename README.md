# OpenDroneMap

Open drone map is a command line tool that will create point clouds, orthomosaics, and DEMs from drone imagery using the SfM workflow

https://docs.opendronemap.org/

The easiest way to run ODM is to run it from an existing docker image which is found on dockerhub
`opendronemap/odm`


```
docker run -ti --rm -v /home/jgillan/Documents/PVCC_hole17/green:/datasets/code  opendronemap/odm --project-path /datasets --skip-orthophoto --skip-report --pc-las --pc-copc 
```

This is what the command is doing:

* Run docker
* Not sure what flags `-it` do exactly
* `--rm` removes image after is has been run
* `-v` mounts a volume. In this case, it is directory on my local machine (/home/jgillan/Documents/PVCC_hole17/green). Within this directory there must be a sub-directory called `/images`. This is where ODM will look for all of the raw drone images. 
* The directory is mounted to the directory `/datasets/code` inside the image
* `opendronemap/odm` is the name of the Docker image which is located on Dockerhub
* The rest of the arguments are flags and options. These are parameters to change how the processing is done. By default, ODM will run through the entire processing pipeline unless you say otherwise. Check out options at https://docs.opendronemap.org/arguments/



## Random Docker Things
Look at the directory structure inside a docker image
```
docker run --rm -it --entrypoint=/bin/bash opendronemap/odm
```
Look at the directory structure inside a running container
```
docker exec -it <container_id_or_name> sh
```

## Pipeline Exploration
I am trying to create a pipeline of sequential containers that creates drone imagery products and then does some point cloud analysis. The first container is `opendronemap/odm` and the second container is `jeffgillan/pdal_copc:1.0`. I want outputs from the first container (point clouds .las) to serve as inputs for the second container (some PDAL analysis). I am exploring the use of docker-compose to create this pipeline. 

A `docker-compose.yml` is used to orchestrate this pipeline

Here is my current build, with no idea what is correct or incorrect 
```
ersion: "3.9"
services:
  odm:
    image: opendronemap/odm
    volumes:
      - /home/jgillan/Documents/PVCC_hole17/green:/datasets/code
    entrypoint: ["python3", "/code/run.py"]
    command: --project-path /datasets/code --skip-orthophoto --skip-report --pc.las --pc.copc

  pdal_copc:
    image: jeffgillan/pdal_copc:0.3
    depends_on:
      - odm
    volumes:
      - /home/jgillan/Documents/PVCC_hole17/green/odm_georeferencing:/data
    entrypoint: ["./pdal_copc.sh"]
    command: >
      -c "wait-for-it.sh odm:/code/odm_orthophoto/done.txt"
```      
I have to use a `wait-for-it.sh` script to ensure the second container does not start until the first is complete. I think this wait-for-it script needs to be placed inside of the second container.  

Here is `wait-for-it.sh`. It was generated by gpt4, so I don't know what is correct. 
```
#!/bin/bash

file_path="$1"
shift

timeout=60
interval=2

counter=0
until [ -f "$file_path" ] || [ "$counter" -ge "$timeout" ]; do
    sleep $interval
    counter=$((counter + interval))
done

if [ -f "$file_path" ]; then
    echo "File $file_path is available, proceeding with the rest of the commands."
    exec "$@"
else
    echo "Timeout reached: File $file_path is still not available."
    exit 1
fi
```
